{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For creating our Neural Network class, we will use the numpy library.\n",
    "Let's also import matplotlib to plot our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Layers\n",
    "\n",
    "To improve modularity, we will divide our network into sequence of **Layers**.\n",
    "Each layer will be responsible for calculating a small part of our network.\n",
    "\n",
    "A typical two-layer neural network is set-up as follows:\n",
    "\n",
    "Input -> First Linear Layer -> Activation Function -> Second Linear Layer -> Output\n",
    "\n",
    "A forward pass in such a network involves flow of the input data from the *Input* to the *Output* layers.\n",
    "\n",
    "A backward pass involves calculating the gradient of the error in the output layer, with respect to the output of the last layer and propagating that gradient through each from the *Output* to the *Input* layer.\n",
    "\n",
    "Each layer can have a number of parameters and is responsible for calculating the error-gradient with respect to those parameters during the backward pass. They can then using those gradients to adjust their parameters as part of training using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    A layer in the network can do one of the following four things:\n",
    "    \n",
    "    * forward pass: Accepts an input and generates an output.\n",
    "    * backward pass: Accepts the gradient of a variable (usually Error) with respect to the output variable\n",
    "                     and uses the chain rule to calculate the gradient with respect to the input variable\n",
    "                     for this layer.                     \n",
    "    * init_params: Initialze any parameters involved in this layer.\n",
    "    * update_params: Update any parameters involved in this layer as part of the training process,\n",
    "                     with the learning rate alpha.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backpropagate(self, next_gradient):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def init_params(self):\n",
    "        pass\n",
    "        \n",
    "    def update_params(self, alpha):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer\n",
    "\n",
    "This layer represents the linear connections of the output of the previous layer to this layer.\n",
    "The connections have weights, represented by the $W$ matrix. Furthermore, a bias term $b$ is introduced. The output of this layer is then calculated as follows:\n",
    "\n",
    "$$\n",
    "Z = WX + b\n",
    "$$\n",
    "\n",
    "Following this equation, suppose we have the gradient of the error with respect to the output $Z$, represented as $\\frac{\\partial E}{\\partial Z}$, then we can calculate the gradient with respect to the input $X$ as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial X} = \\left(W^T \\right) \\left(\\frac{\\partial E}{\\partial Z}\\right)\n",
    "$$\n",
    "\n",
    "Furthermore, to update the parameters $W$ and $b$, we can calulate the error gradient with respect to these parameters as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial W} = \\left(\\frac{\\partial E}{\\partial Z}\\right) \\left(X^T \\right)\n",
    "\\\\\n",
    "\\frac{\\partial E}{\\partial b} = \\left(\\frac{\\partial E}{\\partial Z}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \"\"\"\n",
    "    A linear layer calculates the linear equation WX + b,\n",
    "    where W is the weight matrix, b is the bias and X is the input matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \"\"\"\n",
    "        input_dim: Dimension of each input data for this layer.\n",
    "        output_dim: Dimension of each output data for this layer.\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # This is optional, but we initialize W and b when the layer is created.\n",
    "        self.init_params()\n",
    "        \n",
    "    def forward(self, input_data):        \n",
    "        # Calculate and return WX + b\n",
    "        self.input = input_data\n",
    "        return self.w * self.input + self.b\n",
    "        \n",
    "    def backward(self, next_gradient):\n",
    "        # Besides calculating gradient with respect to the input (X),\n",
    "        # we will also record the gradient with respect to the paramters (W and b) so\n",
    "        # that we can update the parameters using these gradients.\n",
    "        \n",
    "        # Here, the output of this layer: Z = WX + B\n",
    "        # next_gradient = Grad(E, Z)\n",
    "    \n",
    "        # Grad(E, W) = Grad(E, Z) Transpose(X)\n",
    "        self.dw = next_gradient * self.input.T\n",
    "        \n",
    "        # Grad(E, b) = Grad(E, Z)\n",
    "        # but since b is a column vector and next_gradient is a matrix, we will take sum of its columns.\n",
    "        self.db = np.sum(next_gradient, axis=1)\n",
    "        \n",
    "        # Finally, Grad(E, Input=X) = Transpose(W) Grad(E, Z)\n",
    "        return self.w.T * next_gradient\n",
    "    \n",
    "    def init_params(self):\n",
    "        # Use Xavier Initialization:\n",
    "        variance = 2 / (self.output_dim + self.input_dim)\n",
    "        mean = 0\n",
    "        sigma = np.sqrt(variance)\n",
    "        \n",
    "        # Dim(W) = output_dim X input_dim\n",
    "        self.w = np.matrix(\n",
    "            np.random.normal(mean, variance,\n",
    "                             (self.output_dim, self.input_dim))\n",
    "        )\n",
    "        \n",
    "        # Dim(b) = output_dim X b\n",
    "        self.b = np.matrix(\n",
    "            np.random.normal(mean, variance,\n",
    "                             (self.output_dim, 1))\n",
    "        )\n",
    "    \n",
    "    def update_params(self, alpha):\n",
    "        # Use gradient descent to update the parameters.\n",
    "        # Theta_new = Theta_old - Learning_rate * Gradient(E, Theta)\n",
    "        self.w = self.w - alpha * self.dw\n",
    "        self.b = self.b - alpha * self.db\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Layer\n",
    "\n",
    "This layer represents a simple mathematical function used to activate the neuron.\n",
    "\n",
    "For an input $Z$, it simply calculates:\n",
    "\n",
    "$$\n",
    "A = f(Z)\n",
    "$$\n",
    "\n",
    "where $f(Z)$ can be sigmoid, ReLu or any other activation function and is applied element-wise.\n",
    "\n",
    "Given the gradient of the error with respect to it's output $A$, represented as $\\frac{\\partial E}{\\partial A}$, we can calculate the gradient with respect to it's input $Z$ as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial Z} = \\left(\\frac{\\partial E}{\\partial A}\\right) * \\left(f'(Z)\\right)\n",
    "$$\n",
    "\n",
    "where $*$ represents element-wise multiplication and $f'(Z) = \\frac{\\partial f(Z)}{\\partial Z}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    \"\"\"\n",
    "    An activation layer uses a mathematical function to transform input to the output.\n",
    "    \n",
    "    Each activation layer implements the following two functions:\n",
    "    \n",
    "    * apply: Apply the function to the input data.\n",
    "    * apply_d: Apply the derivative of the function with respect to the input variable for given input data.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        # Forward pass is simply applying the function f(Z) to the input variable Z, element-wise.\n",
    "        self.input = input_data\n",
    "        return self.apply(input_data)\n",
    "    \n",
    "    def backward(self, next_gradient):\n",
    "        # Here, the output of this layer: A = f(Z)\n",
    "        # next_gradient = Grad(E, A)\n",
    "        \n",
    "        # First calculate f'(Z)\n",
    "        f_dash = self.apply_d(self.input)\n",
    "        \n",
    "        # Finally, Grad(E, Input=Z) = Grad(E, A) * f'(Z)\n",
    "        # which is element-wise multiplication.\n",
    "        return np.multiply(f_dash, next_gradient)\n",
    "    \n",
    "    def apply(self, data):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def apply_d(self, data):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are two common activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Activation):\n",
    "    \"\"\"\n",
    "    The Sigmoid activation function:\n",
    "    \n",
    "    f(Z) = 1 / (1 + exp(-z))\n",
    "    f'(Z) = f(Z) (1 - f(Z))\n",
    "    \"\"\"\n",
    "    def apply(self, data):\n",
    "        return 1 / (1 + np.exp(-data))\n",
    "    \n",
    "    def apply_d(self, data):\n",
    "        y = self.apply(data)\n",
    "        return np.multiply(y, 1 - y)\n",
    "    \n",
    "\n",
    "class ReLu(Activation):\n",
    "    \"\"\"\n",
    "    The ReLu activation function:\n",
    "    \n",
    "    f(Z) = 0 if Z <= 0 and Z otherwise.\n",
    "    f'(Z) = 0 if Z <=0 and 1 otherwise.\n",
    "    \"\"\"\n",
    "    def apply(self, data):\n",
    "        y = np.copy(data)\n",
    "        y[y <= 0] = 0\n",
    "        return y\n",
    "    \n",
    "    def apply_d(self, data):\n",
    "        y = np.ones(data.shape)\n",
    "        y[data <= 0] = 0\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Feed-Forward Neural Network\n",
    "\n",
    "We can create any architecture of neural network using the above layers. To use a sequence of layers for multiclass classification purpose, we will setup our neural network with following features:\n",
    "\n",
    "**Softmax output**\n",
    "\n",
    "The output of the network is N-size vectors where N is the number of classes and the vector elements are unbounded numbers. Since we have multiple classes to output, we will transform these unbounded numbers to probability of being in different classes using the softmax approach. The softmax function is given as:\n",
    "    \n",
    "$$\n",
    "p(y_i) = \\frac{e^{y_i}}{\\sum_{j=1}^N{e^{y_j}}}\n",
    "$$\n",
    "    \n",
    "**Mini-batch training**\n",
    "\n",
    "To train the data, we will iterate over some given number of epochs and use gradient descent with backpropagation. However, using one input-output training pair for training pass will result in really slow convergence (and even slower computation in python). But since the training size can be very large, we might not be able to train using the whole data at once either. So as a compromise, we will separate the training data into mini-batches each of (at most) given size and train one mini-batch at a time.\n",
    "    \n",
    "**Cross-Entropy error**\n",
    "\n",
    "We want to calculate the error of our neural network as it trains to see how it is improved over the training epochs. We also want to calculate the error using some test data, to report the final accuracy of our trained network. For this purpose, since we are dealing with probabilities as output, we will use cross-entropy error. The error of classifying a test data $x$ is given as:\n",
    "    \n",
    "$$\n",
    "Error(x) = \\sum_i{-t_i log(p_i)}\n",
    "$$\n",
    "    \n",
    "where $p_i$ is the i-th output probability from the neural network (calculated using softmax) and $t_i = 1$ if $x$ belong to class $i$ and $t_i = 0$ otherwise. This error is summed over all the test data to calculate total error.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(Layer):\n",
    "    \"\"\"\n",
    "    A simple feed-forward multi-class neural network.\n",
    "    \n",
    "    This network is capable of calculating the probability of an input data to be in one of several classes\n",
    "    using softmax function to calculate the probability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers, num_classes, learning_rate):\n",
    "        \"\"\"\n",
    "        layers: List of neural network layers in sequence.\n",
    "        num_classes: Number of output classes.\n",
    "        learning_rate: Learning rate to use with the gradient descent during training of the network parameters.\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        \"\"\"\n",
    "        Forward pass an input data through each layer in the network and return the output.\n",
    "        \"\"\"\n",
    "        next_data = input_data\n",
    "        for layer in self.layers:\n",
    "            next_data = layer.forward(next_data)\n",
    "        return next_data\n",
    "    \n",
    "    def backward(self, last_gradient):\n",
    "        \"\"\"\n",
    "        Backpropagatet a gradient (usually the error gradient) with respect to the network output\n",
    "        through each layer from the last.\n",
    "        \"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            last_gradient = layer.backward(last_gradient)\n",
    "        return last_gradient\n",
    "        \n",
    "    def init_params(self):\n",
    "        \"\"\"\n",
    "        Initialize each layer's parameters.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.init_params()\n",
    "\n",
    "    def update_params(self, alpha):\n",
    "        \"\"\"\n",
    "        Update each layer's parameters using gradient descent with the learning rate of alpha.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.update_params(alpha)\n",
    "            \n",
    "    def train(self, input_data, target_data, num_epochs, batch_size):\n",
    "        \"\"\"\n",
    "        Train over the given input and target matrices, taking mini-batches of given size and for given\n",
    "        number of epochs.\n",
    "        \"\"\"\n",
    "        \n",
    "        # First transform the input and output matrices to the format acceptable to out network.\n",
    "        # This means, tranposing the input document matrix to get X\n",
    "        # and one-hot encoding the output matrix to get Y.\n",
    "        X, T = self.preprocess(input_data, target_data)\n",
    "        \n",
    "        # At this point, each column of X and T represent one pair of input-output data and the whole matrices\n",
    "        # represent the complete batch.\n",
    "        \n",
    "        num_items = len(input_data)\n",
    "        \n",
    "        # We will collect the errors over each epoch and each mini batch, which is useful for plotting later.\n",
    "        errors = []\n",
    "        batch_errors = []\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # For each epoch, shuffle the training data.\n",
    "            \n",
    "            # Create a shuffled indices array.\n",
    "            indices = np.arange(num_items)\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            # Shuffle the data around. Note that we need to shuffle the columns.\n",
    "            # Also, we must shuffle the X and T both the same way so the corresponding pairs are always together.\n",
    "            X = X[:, indices]\n",
    "            T = T[:, indices]\n",
    "            \n",
    "            \n",
    "            # Error for this epoch.\n",
    "            epoch_error = 0\n",
    "            \n",
    "            # Take each mini-batch.\n",
    "            for i in range(0, num_items, batch_size):\n",
    "\n",
    "                # Feed the mini-batch forward.\n",
    "                y = self.forward(X[:, i:i + batch_size])\n",
    "                # Use softmax to calculate the probabilities of being in different classes.\n",
    "                p = self.softmax(y)\n",
    "                \n",
    "                # The corresponding one-hot encoded target vectors.\n",
    "                t = T[:, i:i + batch_size]\n",
    "    \n",
    "                # Calculate the error between the output and the target for this mini-batch.\n",
    "                error = self.calc_error(p, t)\n",
    "                batch_errors.append(error)\n",
    "                epoch_error += error\n",
    "                \n",
    "                # For cross-entropy error,\n",
    "                # Grad(Error, Output) = p - t\n",
    "                grad_error = (p - t)\n",
    "                \n",
    "                # Back-propagate this gradient to each layer from the last.\n",
    "                self.backward(grad_error)\n",
    "                \n",
    "                # Update the parameters of each layer using gradient descent.\n",
    "                self.update_params(self.learning_rate)\n",
    "                \n",
    "            # Calculate mean error for this epoch.\n",
    "            errors.append(epoch_error / i)\n",
    "                \n",
    "        # Return the errors.\n",
    "        return errors, batch_errors\n",
    "                \n",
    "    def test(self, input_data, target_data):\n",
    "        \"\"\"\n",
    "        Test the network using the pair of input and target matrices and calculate the errors.\n",
    "        Can be used for validation and error reporting.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Transform the input and target matrices just like we do in training.\n",
    "        X, T = self.preprocess(input_data, target_data)\n",
    "        \n",
    "        # Forward pass the input through the network layers.\n",
    "        y = self.forward(X)\n",
    "        \n",
    "        # Calculate the probability of the data to be in the different classes using softmax.\n",
    "        p = self.softmax(y)\n",
    "        \n",
    "        # Calculate the error between the computed probabilites and the target vectors.\n",
    "        return self.calc_error(p, T)\n",
    "            \n",
    "    def preprocess(self, input_data, target_data):\n",
    "        \"\"\"\n",
    "        Given an input document matrix and their classes, create X and T matrices respectively in the format\n",
    "        that can be used with this neural network.\n",
    "        \n",
    "        input_data: A matrix of input data where each row is a input vector.\n",
    "        target_data: A list/array of numbers representing the class index of the corresponding input vector.\n",
    "        \"\"\"\n",
    "    \n",
    "        num_items = len(input_data)\n",
    "        \n",
    "        # Transpose the input_data to create X matrix.\n",
    "        X = np.matrix(input_data).T\n",
    "        \n",
    "        # Use one-hot encoding to transform the target_data vector into Y matrix.\n",
    "        T = np.zeros((self.num_classes, num_items))\n",
    "        T[target_data, np.arange(num_items)] = 1\n",
    "        T = np.matrix(T)\n",
    "        \n",
    "        # Return X and T.\n",
    "        return X, T\n",
    "    \n",
    "    def softmax(self, y):\n",
    "        \"\"\"\n",
    "        Use softmax function on the given matrix, over each column, to convert the values into probabilities.\n",
    "        \"\"\"\n",
    "        # The real softmax function is:\n",
    "        # exps = np.exp(y)\n",
    "        # p = exps / Sum(exps)\n",
    "        \n",
    "        # However, sometimes the values of y can be very large and applying the exponential function\n",
    "        # can result in overflow errors.\n",
    "        \n",
    "        # So, we can use a trick to get the same result, by subtracting a large value from each value in y.\n",
    "        # For our purpose, we can subtract the maximum value in each y-column and take the exp and\n",
    "        # get the same result without overflowing.\n",
    "        exps = np.exp(y - y.max(axis=0))  # Originally: exps = np.exp(y)\n",
    "        return exps / exps.sum(axis=0)\n",
    "    \n",
    "    def calc_error(self, y, t):\n",
    "        \"\"\"\n",
    "        Given an output probability matrix y and corresponding target class matrix (one-hot encoded) t,\n",
    "        calculate the error.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Cross Entropy Error given by:\n",
    "        # Error = 1 / M * Sum(-t log(y))\n",
    "        j = np.sum(-np.multiply(t, np.log(y)))\n",
    "        return float(j) / t.size\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's test the network using the MNIST dataset. First create a Two-Layer neural network using 100 neurons and sigmoid activation in the hidden layer. The input and output size for the MNIST dataset is 784 and 10 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network([\n",
    "    Linear(784, 100),\n",
    "    Sigmoid(),\n",
    "    Linear(100, 10),\n",
    "], 10, 0.1)  # Learning rate = 0.1\n",
    "\n",
    "network.init_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the mnist dataset and train over them. Note that normalizing the inputs to $[0, 1]$ range will help smooth out the training process because we don't want large values for sigmoid activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist\n",
    "\n",
    "x_train, t_train, x_test, t_test = mnist.load()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "errors, batch_errors = network.train(x_train, t_train, 24, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training errors, we can see how the network is converging towards the point where there's minimal error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6dac0aed00>]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('Training Batch')\n",
    "plt.ylabel('Error')\n",
    "plt.plot(batch_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cleaner plot is obtained when using mean errors over each epoch instead of each mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6dacaa4760>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('Training Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.plot(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the accuracy of our network over a test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.43133006784201%\n"
     ]
    }
   ],
   "source": [
    "accuracy = 100 - network.test(x_test, t_test) * 100\n",
    "print(f'Accuracy: {accuracy}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad for simple feed forward network, specially since the inputs are images and we can probably do even better with a convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
